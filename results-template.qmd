---
title: "results template"
format: pdf
editor: visual
---

```{r}
#| echo: false
#| output: false

library(readr)
library(dplyr)
library(stringr)
library(lme4)
library(lmerTest) # stack overflow said this would show p values
library(ggplot2)
res <- read_csv(file = "results_prod 102.csv",col_names = FALSE) # need to download from google sheets

df <- res
head(df)
colnames(df) <- LETTERS[1:24] # like the google sheet
df <- filter(df, !grepl("^#",A)) # remove all rows that start with a hashtag which are all the comments
head(df)
colnames(df) <- c("subj", "IP", "controller", "order", "element", "label", "latin", "penntype", "pennname","parameter","value","eventTime","prolificID","num", "item", "group", "isCrit", "isLeg", "sentNum", "text", "author", "fileName", "comments")

# filter out idan response and any participants above 50 and the first two responses
df <- filter(df, subj != "1744586734" & subj != "1744586986" & subj != "1744662429") # removes first two subject ids and idans subject id (subject id is just the time of when participant started the experiment which is unique)

df <- filter(df, !(subj %in% unique(df$subj)[filter(df, parameter == "age")$value > 50])) # filter out participants whose age is above 50



df <- select(df, -c("IP","controller","order", "element", "latin","penntype","eventTime", "prolificID", "comments"))

filter(df, value == "unchecked") # what to do w people who don't check native english speaker???


df <- filter(df, label != "instructions" & pennname == "q1" | pennname == "demographics")
df$subj <- as.factor(df$subj)


# exclusion critera
str_detect(paste(df_main$isCrit,collapse=""),"critcrit") # if anyone got 2 criticals in a row




# SPLIT INTO DEMOGRAPHICS AND MAIN STUDY
df_main <- filter(df,pennname == "q1")
df_demo <- filter(df,pennname == "demographics")


df_main$value <- gsub("%2C","",df_main$value) # replace %2C (commas) with nothing because they are not important
df_main$value <- gsub("\\.","",df_main$value) # remove periods because they are not important
df_main$value <- gsub("’","",df_main$value) # remove weird apostrophes because they mess everything up
df_main$value <- gsub("'","",df_main$value) # remove normal apostrophes because they mess everything up
df_main$value <- tolower(df_main$value) # make all same case
df_main$value <- gsub(" ","",df_main$value) # remove all spaces


# do the same for text column
df_main$text <- gsub("%2C","",df_main$text) # replace %2C (commas) with nothing because they are not important
df_main$text <- gsub("\\.","",df_main$text) # remove periods because they are not important
df_main$text <- gsub("’","",df_main$text) # remove weird apostrophes because they mess everything up
df_main$text <- gsub("'","",df_main$text) # remove apostrophes because they mess everything up
df_main$text <- tolower(df_main$text) # make all same case
df_main$text <- gsub(" ","",df_main$text) # remove all spaces


df_main$sent <- as.numeric(gsub("_","",df_main$sentNum))


df_main_crit <- filter(df_main, isCrit == "crit")


df_main_crit$leg <- ifelse(df_main_crit$isLeg == "leg",0,1)
# # leg = 0 , ill = 1

df_main_crit$sim <- ifelse(df_main_crit$sent == 1 | df_main_crit$sent == 2 | df_main_crit$sent == 5 | df_main_crit$sent == 6 | df_main_crit$sent == 9 | df_main_crit$sent == 12 | df_main_crit$sent == 16 | df_main_crit$sent == 17 | df_main_crit$sent == 18 | df_main_crit$sent == 20, 0,1)
# # sim = 0, dis = 1
# 

changed_df <- select(df_main_crit, c("subj","sent","value", "text", "leg", "sim"))
changed_df$isNonLiteral <- NA

rows_to_discard <- NA

crit_words_implaus <- c("banks","smell","gold","desk","map","saved","ball","hit","naps","run","blamed","ties","liver","rain","halted","warm","water","hear","dining","meat")
crit_words_plaus <- c("barks","swell","mold","disk","mop","sawed","mall","lit","nags","sun","flamed","tics","liner","gain","halved","warn","wafer","near","pining","moat")


for (i in 1:nrow(changed_df[1:10,])) {
  cur_value <- changed_df$value[i]
  cur_text <- changed_df$text[i]
  cur_sent <- changed_df$sent[i]
  cur_implaus_word <- crit_words_implaus[i]
  cur_plaus_word <- crit_words_plaus[i]
  if (cur_value == cur_text) { # the sentence is re-typed correctly (e.g., the typed string with no spaces and punctuation matches the original sentences with not spaces and punctuation), then count it as literal

    changed_df$isNonLiteral <- 0 # literal
  }
  else if () { # the typed sentence matches the “plausible” version of the sentence, count it as non-literal

    
  } else if (!(str_detect(cur_value,cur_implaus_word) | str_detect(cur_value,cur_plaus_word))) { # neither the critical implausible word nor the critical plausible word are found as a substring of the typed string, discard the trial

    rows_to_discard <- c(rows_to_discard, i)
  } else {
    
  }
}


#now calculate literal/nonliteral (changed)
# literal = 0, non literal = 1
# variable is isNonLiteral (changed)


# * IF the sentence is re-typed correctly (e.g., the typed string with no spaces and punctuation matches the original sentences with not spaces and punctuation), 
# then count it as literal
# 
# * Else, IF the typed sentence matches the “plausible” version of the sentence, count it as non-literal
# 
# * Else, IF neither the critical implausible word nor the critical plausible word are found as a substring of the typed string, discard the trial
# 
# * Else, print the trial for inspection and pause for user input - these are cases where either the plausible or implausible words are typed, but something else was typed incorrectly. I think it would be the best to look at those one by one to decide how to code them. Some of them might just have typos that we can ignore and count as correct (e.g, we have a participant who types the name “Tony” and “Tonyt”); others might be cases where the participants truly mis-read/mis-interpreted another word in the sentence, in which case we should probably discard the trial. So, each of these sentences would be printed out by your code, and then you can choose one of three responses - e.g., 1 (count as literal), 2 (count as non-literal), or 3 (discard trial).


# df_main_crit_q1_sent$changed <- ifelse(df_main_crit_q1_sent$Data == df_main_crit_q1_sent$text,0,1)
# # LOOK BACK AT THIS
# # remove all where something other than plaus or implaus?
# 
# final_df <- df_main_crit
# 
# # GLMER
# # need to generate coeficients from the model without an interaction in gibson study
# model <- glmer(changed ~ 1 + leg + sim +# because generated with interaction, can still run a model without an interaction
#                  + (1 | subj) 
#                  + (0 + sim | subj)
#                  + (0 + leg | subj)
#                  + (1 | sent),data = final_df,family="binomial")
# update(model, control = glmerControl(calc.derivs = FALSE))
# summary(model)
# # first fixed effects, then the effects of the variables on y varying across the subj/sent
# 
# # Interaction Model
# # effect of legibility when sim is 0, but not coded as 0 so cannot interpret these coef
# model_interaction <- glmer(changed ~ 1 + leg * sim + 
#                  + (1 | subj) 
#                + (0 + sim | subj)
#                + (0 + leg | subj)
#                + (1 | sent),data = final_df,family="binomial")
# update(model_interaction, control = glmerControl(calc.derivs = FALSE))
# summary(model_interaction)
# 
# 
# 
# #Interaction model with Sim = 0
# # for the simple effects, the effect when similar is coded as 0
# # when the letter is similar, people are more likely to change illegible sentences than legible sentences (because the estimate is positive)
# 
# model_interaction_sim_dummy_sim <- glmer(changed ~ 1 + leg * sim_dummy_sim  
#                            + (1 | subj) 
#                            + (0 + sim_dummy_sim | subj)
#                            + (0 + leg | subj)
#                            + (1 | sent),data = final_df,family="binomial")
# update(model_interaction_sim_dummy_sim, control = glmerControl(calc.derivs = FALSE))
# summary(model_interaction_sim_dummy_sim)
# 
# 
# # Interaction model with Sim = 1
# # for the simple effects, the effect when similar is coded as 1, and dissimilar is coded as 0
# # when the letter is similar, people are more likely to change illegible sentences than legible sentences (because the estimate is positive)
# # the difference between the simple effects is not significant (interaction is not significant)
# model_interaction_sim_dummy_dis <- glmer(y ~ 1 + leg * sim_dummy_dis + 
#                              + (1 | subj) 
#                            + (0 + sim_dummy_dis | subj)
#                            + (0 + leg | subj)
#                            + (1 | sent),data = df,family="binomial")
# update(model_interaction_sim_dummy_dis, control = glmerControl(calc.derivs = FALSE))
# summary(model_interaction_sim_dummy_dis)
# 
# 
# 
# # summary
# summglm <- summary(model)
# summglm
# summglm$vcov
# summglm$coefficients
# 
# 
# # estimates
# beta0_est <- unname(fixef(model)[1])
# beta1_est <- unname(fixef(model)[2])
# beta2_est <- unname(fixef(model)[3])
# 
# varcorr_est <- VarCorr(model)
# 
# 
# t00_est <- varcorr_est$subj[1] # variance of random intercept of subject
# #t01_est <- varcorr_est_subj[1,2] # covariance between intercept and slope
# t11_est <- varcorr_est$subj.1[1] # variance of random slope of legibility by subject
# t22_est <- varcorr_est$subj.2[1] # variance of random slope of similarity by participant
# 
# 
# xi00_est <- varcorr_est$sent[1] # variance of sentence intercepts?
# 
# 
# # Difference of "population" parameters vs estimated parameters
# # dif <- cbind(beta0-beta0_est,beta1-beta1_est,beta2-beta2_est,
# #              t00-t00_est,t11-t11_est,t22-t22_est,
# #              xi00-xi00_est)
# # colnames(dif) <- c("beta0","beta1","beta2","t00","t11","t22","xi")
# # dif
```

## Results

### Hypothesis 1: effect of legibility

When participants read illegible sentences, they were r ifelse(sign(beta1_est) == 1,"more","less") likely to change the sentence (b, se) 

### Hypothesis 2: effect of similarity

### Hypothesis 3: interaction





### demographics

```{r}
#| echo: false
#| output: false

num_participants <- length(unique(df_demo$subj))
mean_age <- mean(as.numeric(filter(df_demo, parameter == "age")$value))
sd_age <- sd(as.numeric(filter(df_demo, parameter == "age")$value))
num_female <- sum(filter(df_demo, parameter == "gender")$value == "female")
num_male <- sum(filter(df_demo, parameter == "gender")$value == "male")

```

The mean age of the `r num_participants` participants is `r mean_age` the standard deviation is `r sd_age`. There are `r num_female` females and `r num_male` males.
